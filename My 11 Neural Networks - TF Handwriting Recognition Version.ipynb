{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9e8b519",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96b80363",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import seed\n",
    "seed(888)\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.set_random_seed(404)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d4ba96b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from time import strftime\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a3713f",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6e30733c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_TRAIN_PATH = 'MNIST/digit_xtrain.csv'\n",
    "X_TEST_PATH = 'MNIST/digit_xtest.csv'\n",
    "Y_TRAIN_PATH = 'MNIST/digit_ytrain.csv'\n",
    "Y_TEST_PATH = 'MNIST/digit_ytest.csv'\n",
    "\n",
    "LOG_PATH = 'tensorboard_mnist_digit_logs/'\n",
    "LOG_PATH2 = 'tensorboard_mnist_digit_logs2/'\n",
    "LOG_PATH3 = 'tensorboard_mnist_digit_logs3/'\n",
    "\n",
    "NR_CLASSES = 10\n",
    "VALIDATION_SIZE = 10000\n",
    "IMAGE_WIDTH = 28\n",
    "IMAGE_HEIGHT = 28\n",
    "CHANNELS = 1\n",
    "TOTAL_INPUTS = IMAGE_WIDTH*IMAGE_HEIGHT*CHANNELS\n",
    "\n",
    "TEST_IMAGE = 'MNIST/test_img.png'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc70813",
   "metadata": {},
   "source": [
    "# Get the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "711a9f9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 387 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "y_train_all = np.loadtxt(Y_TRAIN_PATH, delimiter=',', dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "767ef6e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b8f56a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = np.loadtxt(Y_TEST_PATH, delimiter=',', dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4f53363",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aedd96f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 30.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "x_train_all = np.loadtxt(X_TRAIN_PATH, delimiter=',', dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "032d0095",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 5.02 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "x_test = np.loadtxt(X_TEST_PATH, delimiter=',', dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "17ba361c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 784)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ba75dddf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 784)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aec3fee",
   "metadata": {},
   "source": [
    "# Explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "456e8fcc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784,)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_all[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "081ccedc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 0, 4, 1, 9])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_all[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a84772d",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "86655bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re_scale\n",
    "\n",
    "x_train_all, x_test = x_train_all / 255.0, x_test / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b7bdc8",
   "metadata": {},
   "source": [
    "#### Convert target values to one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "10e2cde7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values =  y_train_all[:5]\n",
    "np.eye(10)[values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ac980667",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.eye(10)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5771c7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_all = np.eye(NR_CLASSES)[y_train_all]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "46a1ff88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 10)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "81a0629d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = np.eye(NR_CLASSES)[y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ff7b5cf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 10)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf87cf18",
   "metadata": {},
   "source": [
    "### Create validation dataset from training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "deaab759",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val = x_train_all[:VALIDATION_SIZE]\n",
    "y_val = y_train_all[:VALIDATION_SIZE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "444cf50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train_all[VALIDATION_SIZE:]\n",
    "y_train = y_train_all[VALIDATION_SIZE:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "977f94c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 784)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3ebfce7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 784)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782cd51b",
   "metadata": {},
   "source": [
    "# Setup Tensorflow Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "c1312485",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.compat.v1.disable_eager_execution()\n",
    "X = tf.compat.v1.placeholder(tf.float32, shape=[None, TOTAL_INPUTS], name='X')\n",
    "Y = tf.compat.v1.placeholder(tf.float32, shape=[None, NR_CLASSES], name='labels')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e47475",
   "metadata": {},
   "source": [
    "### Neural Network Architecture\n",
    "\n",
    "#### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "e9e1d6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "nr_epochs = 50\n",
    "learning_rate = 1e-3\n",
    "\n",
    "n_hidden1 = 512\n",
    "n_hidden2 = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "d141127e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_layer(input, weight_dim, bias_dim, name):\n",
    "     with tf.compat.v1.name_scope(name):\n",
    "        initial_w = tf.compat.v1.truncated_normal(shape=weight_dim, stddev=0.1, seed=42)\n",
    "        w = tf.compat.v1.Variable(initial_value=initial_w, name='W') \n",
    "\n",
    "        initial_b = tf.compat.v1.constant(value=0.0, shape=bias_dim) \n",
    "        b = tf.compat.v1.Variable(initial_value=initial_b, name='B')\n",
    "\n",
    "        layer_in = tf.compat.v1.matmul(input, w) + b\n",
    "        \n",
    "        if name=='out':\n",
    "            layer_out = tf.compat.v1.nn.softmax(layer_in)\n",
    "        else:\n",
    "            layer_out = tf.compat.v1.nn.relu(layer_in)\n",
    "        \n",
    "        tf.compat.v1.summary.histogram('weights', w)\n",
    "        tf.compat.v1.summary.histogram('biases', b)\n",
    "        \n",
    "        return layer_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "f07c4a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model without dropout\n",
    "# layer_1 = setup_layer(X, weight_dim=[TOTAL_INPUTS, n_hidden1], \n",
    "#                       bias_dim=[n_hidden1], name='layer_1')\n",
    "# layer_2 = setup_layer(layer_1, weight_dim=[n_hidden1, n_hidden2], \n",
    "#                       bias_dim=[n_hidden2], name='layer_2')\n",
    "# output = setup_layer(layer_2, weight_dim=[n_hidden2, NR_CLASSES], \n",
    "#                      bias_dim=[NR_CLASSES], name='out')\n",
    "\n",
    "# model_name = f'{n_hidden1}-{n_hidden2} LR{learning_rate} E{nr_epochs}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "f82d4b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_1 = setup_layer(X, weight_dim=[TOTAL_INPUTS, n_hidden1], \n",
    "                      bias_dim=[n_hidden1], name='layer_1')\n",
    "layer_drop = tf.compat.v1.nn.dropout(layer_1, keep_prob=0.8, name='dropout_layer')\n",
    "\n",
    "layer_2 = setup_layer(layer_drop, weight_dim=[n_hidden1, n_hidden2], \n",
    "                      bias_dim=[n_hidden2], name='layer_2')\n",
    "output = setup_layer(layer_2, weight_dim=[n_hidden2, NR_CLASSES], \n",
    "                     bias_dim=[NR_CLASSES], name='out')\n",
    "\n",
    "model_name = f'{n_hidden1}-DO-{n_hidden2} LR{learning_rate} E{nr_epochs}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f93ad50",
   "metadata": {},
   "source": [
    "# Tensorboard Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "9f4dea34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created directories!\n"
     ]
    }
   ],
   "source": [
    "# Folder for Tensorboard\n",
    "\n",
    "folder_name = f'{model_name} at {strftime(\"%H %M\")}'\n",
    "directory = os.path.join(LOG_PATH3, folder_name)\n",
    "\n",
    "try:\n",
    "    os.makedirs(name=directory)\n",
    "except OSError as exception:\n",
    "    print(exception.strerror)\n",
    "else:\n",
    "    print('Successfully created directories!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a8be2f",
   "metadata": {},
   "source": [
    "# Loss Optimisation and Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b48485d",
   "metadata": {},
   "source": [
    "#### Defining Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "c4692cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.compat.v1.name_scope('loss_calc'):\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=Y, logits=output))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a002a3",
   "metadata": {},
   "source": [
    "#### Defining Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "b34a6630",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.compat.v1.name_scope('optimizer'):\n",
    "    optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    train_step = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f76e07b",
   "metadata": {},
   "source": [
    "#### Accuracy Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "45a95ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.compat.v1.name_scope('accuracy_calc'):\n",
    "    correct_pred = tf.equal(tf.argmax(output, axis=1), tf.argmax(Y, axis=1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "cfbad6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.compat.v1.name_scope('perfomance'):\n",
    "    tf.compat.v1.summary.scalar('accuracy', accuracy) # Creating summary for tensorboard.\n",
    "    tf.compat.v1.summary.scalar('cost', loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6127087c",
   "metadata": {},
   "source": [
    "#### Check input images on Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "0881417a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.compat.v1.name_scope('show_image'):\n",
    "    x_image = tf.compat.v1.reshape(X, [-1, 28, 28, 1])\n",
    "    tf.compat.v1.summary.image('image_input', x_image, max_outputs=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ea523a",
   "metadata": {},
   "source": [
    "# Run Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "40f5826d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.compat.v1.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e256ff",
   "metadata": {},
   "source": [
    "### Setup Filewriter and Merge Summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "6e9fb622",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging all summaries\n",
    "\n",
    "merged_summary = tf.compat.v1.summary.merge_all()\n",
    "\n",
    "# Creating a filewriter that writes files for tensorboard to read.\n",
    "\n",
    "train_writer = tf.compat.v1.summary.FileWriter(directory + '/train')\n",
    "\n",
    "# Giving filewriter the location of the calculations to write on the file that will be read by tensorboard.\n",
    "\n",
    "train_writer.add_graph(sess.graph)\n",
    "\n",
    "# creating filewriter for the validation data.\n",
    "\n",
    "validation_writer = tf.compat.v1.summary.FileWriter(directory + '/validation')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156e7268",
   "metadata": {},
   "source": [
    "# Initialise all the variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "b17126d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.compat.v1.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32dac106",
   "metadata": {},
   "source": [
    "# Batching the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "518fe71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "size_of_batch = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "70b48de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_examples = y_train.shape[0]\n",
    "nr_iterations = int(num_examples/size_of_batch)\n",
    "\n",
    "index_in_epoch = 0    # This will help us keep track of where one batch ends and the new batch starts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "ac403ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_batch(batch_size, data, labels):\n",
    "    \n",
    "    global num_examples\n",
    "    global index_in_epoch\n",
    "    \n",
    "    start = index_in_epoch\n",
    "    index_in_epoch += batch_size\n",
    "    \n",
    "    if index_in_epoch > num_examples:\n",
    "        start = 0\n",
    "        index_in_epoch = batch_size\n",
    "    \n",
    "    end = index_in_epoch\n",
    "    \n",
    "    return data[start:end], labels[start:end]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1701e0e",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "a8fc3007",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 \t| Training Accuracy = 0.8479999899864197\n",
      "Epoch 1 \t| Training Accuracy = 0.8539999723434448\n",
      "Epoch 2 \t| Training Accuracy = 0.8650000095367432\n",
      "Epoch 3 \t| Training Accuracy = 0.8690000176429749\n",
      "Epoch 4 \t| Training Accuracy = 0.875\n",
      "Epoch 5 \t| Training Accuracy = 0.8759999871253967\n",
      "Epoch 6 \t| Training Accuracy = 0.9769999980926514\n",
      "Epoch 7 \t| Training Accuracy = 0.9750000238418579\n",
      "Epoch 8 \t| Training Accuracy = 0.984000027179718\n",
      "Epoch 9 \t| Training Accuracy = 0.9779999852180481\n",
      "Epoch 10 \t| Training Accuracy = 0.9819999933242798\n",
      "Epoch 11 \t| Training Accuracy = 0.984000027179718\n",
      "Epoch 12 \t| Training Accuracy = 0.9860000014305115\n",
      "Epoch 13 \t| Training Accuracy = 0.9869999885559082\n",
      "Epoch 14 \t| Training Accuracy = 0.9890000224113464\n",
      "Epoch 15 \t| Training Accuracy = 0.9890000224113464\n",
      "Epoch 16 \t| Training Accuracy = 0.9909999966621399\n",
      "Epoch 17 \t| Training Accuracy = 0.9900000095367432\n",
      "Epoch 18 \t| Training Accuracy = 0.9890000224113464\n",
      "Epoch 19 \t| Training Accuracy = 0.9890000224113464\n",
      "Epoch 20 \t| Training Accuracy = 0.9879999756813049\n",
      "Epoch 21 \t| Training Accuracy = 0.9879999756813049\n",
      "Epoch 22 \t| Training Accuracy = 0.9929999709129333\n",
      "Epoch 23 \t| Training Accuracy = 0.9919999837875366\n",
      "Epoch 24 \t| Training Accuracy = 0.9929999709129333\n",
      "Epoch 25 \t| Training Accuracy = 0.9940000176429749\n",
      "Epoch 26 \t| Training Accuracy = 0.9900000095367432\n",
      "Epoch 27 \t| Training Accuracy = 0.9919999837875366\n",
      "Epoch 28 \t| Training Accuracy = 0.9929999709129333\n",
      "Epoch 29 \t| Training Accuracy = 0.9919999837875366\n",
      "Epoch 30 \t| Training Accuracy = 0.9929999709129333\n",
      "Epoch 31 \t| Training Accuracy = 0.9919999837875366\n",
      "Epoch 32 \t| Training Accuracy = 0.9929999709129333\n",
      "Epoch 33 \t| Training Accuracy = 0.9929999709129333\n",
      "Epoch 34 \t| Training Accuracy = 0.9940000176429749\n",
      "Epoch 35 \t| Training Accuracy = 0.9940000176429749\n",
      "Epoch 36 \t| Training Accuracy = 0.9929999709129333\n",
      "Epoch 37 \t| Training Accuracy = 0.9929999709129333\n",
      "Epoch 38 \t| Training Accuracy = 0.9909999966621399\n",
      "Epoch 39 \t| Training Accuracy = 0.9929999709129333\n",
      "Epoch 40 \t| Training Accuracy = 0.9919999837875366\n",
      "Epoch 41 \t| Training Accuracy = 0.9940000176429749\n",
      "Epoch 42 \t| Training Accuracy = 0.9919999837875366\n",
      "Epoch 43 \t| Training Accuracy = 0.9929999709129333\n",
      "Epoch 44 \t| Training Accuracy = 0.9940000176429749\n",
      "Epoch 45 \t| Training Accuracy = 0.9929999709129333\n",
      "Epoch 46 \t| Training Accuracy = 0.9940000176429749\n",
      "Epoch 47 \t| Training Accuracy = 0.9929999709129333\n",
      "Epoch 48 \t| Training Accuracy = 0.9940000176429749\n",
      "Epoch 49 \t| Training Accuracy = 0.9929999709129333\n",
      "Done Training\n",
      "Wall time: 3min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for epoch in range(nr_epochs):\n",
    "    #============= Training Dataset =============\n",
    "    for i in range(nr_iterations):\n",
    "        \n",
    "        batch_x, batch_y = next_batch(batch_size=size_of_batch, data=x_train, labels=y_train)\n",
    "        \n",
    "        feed_dictionary = {X:batch_x, Y:batch_y} #This is a batch that will be running.\n",
    "        \n",
    "        sess.run(train_step, feed_dict=feed_dictionary) # here we are running the training on the batch.\n",
    "        \n",
    "    s, batch_accuracy = sess.run(fetches=[merged_summary, accuracy], feed_dict=feed_dictionary) #Calculating the accuracy of the above batch.\n",
    "    \n",
    "    train_writer.add_summary(s, epoch)\n",
    "    \n",
    "    print(f'Epoch {epoch} \\t| Training Accuracy = {batch_accuracy}')\n",
    "    \n",
    "    #============= Validation ===============\n",
    "    \n",
    "    summary = sess.run(fetches=merged_summary, feed_dict={X:x_val, Y:y_val})\n",
    "    validation_writer.add_summary(summary, epoch)\n",
    "\n",
    "print('Done Training')        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c00167",
   "metadata": {},
   "source": [
    "# Make a Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "34eb7226",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAYAAAByDd+UAAABAElEQVR4nO2Vyw2DMAyG3ap3RoAVmMCBrXICbwIbMEEEmzBCmMA9VFR9EGLTqhIV3xET/vjHjxMzM/yQ8y/FDsEgRAREtEnwJC2aWWAYhqfnzjmVoChDIroLISI45+5C2kwvsRemaYK+76HrOkiS5CmGiG8ZR2EB4zgGY0VRcF3Xks8wM7PI0jRNgzFEVCW4j7b4CLH5C3jv2RjD3nvxmWiVhphbJc/zt+pdY5NgWZYAcCuYqqp0hzUWWmvZGMPWWrX9KksfJ03TNKttEmNV8HWkqe1bIDi8iQjatoUsy8TNLbnQ6rbQDObZiej22Pz3F5DMVPE+/Bb/P0sPwf0LXgGAJwNqzP5nHgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=RGBA size=28x28 at 0x1C56C7DEBB0>"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = Image.open(TEST_IMAGE)\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "7e40febe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert image to grayscale and convert it into a flat array.\n",
    "\n",
    "bw = img.convert('L') #This converts the image into black and white."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "d056aef9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28)"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We need to invert the colors so that the writing is white and the background is black.\n",
    "\n",
    "img_array = np.invert(bw)\n",
    "\n",
    "img_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "86382f33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784,)"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We now have to flatten the array. For this we will use the numpy ravel method.\n",
    "\n",
    "test_img = img_array.ravel()\n",
    "\n",
    "test_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "53e2fc2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = sess.run(fetches=tf.compat.v1.argmax(output, axis=1), feed_dict={X:[test_img]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "81c8b54d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for our test image is:[2]\n"
     ]
    }
   ],
   "source": [
    "print(f'Prediction for our test image is:{prediction}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5934d2",
   "metadata": {},
   "source": [
    "# Testing and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "fdf4fdc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy is 98.01%\n"
     ]
    }
   ],
   "source": [
    "test_accuracy = sess.run(fetches=[accuracy], feed_dict={X:x_test, Y:y_test})\n",
    "\n",
    "print(f'Test Accuracy is {test_accuracy[0]:.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5477b8fa",
   "metadata": {},
   "source": [
    "# Reset for the Next Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "41f1dbeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_writer.close()\n",
    "validation_writer.close()\n",
    "sess.close()\n",
    "tf.compat.v1.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7df9e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95c7db1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c622792",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "65b93600",
   "metadata": {},
   "source": [
    "# Code for 1st Part of module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "d65fc3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#with tf.compat.v1.name_scope('hidden_layer1'):\n",
    "\n",
    "#    initial_w1 = tf.compat.v1.truncated_normal(shape=[TOTAL_INPUTS, n_hidden1], stddev=0.1, seed=42) #Creating initial weights\n",
    "#    w1 = tf.compat.v1.Variable(initial_value=initial_w1, name='w1')     #Asigning a variable to hold them. \n",
    "#    # Creating initial Bias\n",
    "#    initial_b1 = tf.compat.v1.constant(value=0.0, shape=[n_hidden1]) \n",
    "#    b1 = tf.compat.v1.Variable(initial_value=initial_b1, name='b1')\n",
    "#    layer1_in = tf.compat.v1.matmul(X, w1) + b1\n",
    "#    layer1_out = tf.compat.v1.nn.relu(layer1_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23972005",
   "metadata": {},
   "source": [
    "# Instructor Challenge\n",
    "\n",
    "### My Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "222710f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#with tf.compat.v1.name_scope('hidden_layer2'):\n",
    "\n",
    "#    initial_w2 = tf.compat.v1.truncated_normal(shape=[n_hidden1, n_hidden2], stddev=0.1, seed=42)\n",
    "#    w2 = tf.compat.v1.Variable(initial_value=initial_w2, name='w2')  \n",
    "\n",
    "#    initial_b2 = tf.compat.v1.constant(value=0.0, shape=[n_hidden2]) \n",
    "#    b2 = tf.compat.v1.Variable(initial_value=initial_b2, name='b2')\n",
    "\n",
    "#    layer2_in = tf.compat.v1.matmul(layer1_out, w2) + b2\n",
    "#    layer2_out = tf.compat.v1.nn.relu(layer2_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "be828b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#with tf.compat.v1.name_scope('output_layer'):\n",
    "\n",
    "#    initial_w3 = tf.compat.v1.truncated_normal(shape=[n_hidden2, NR_CLASSES], stddev=0.1, seed=42)\n",
    "#    w3 = tf.compat.v1.Variable(initial_value=initial_w3, name='w3') \n",
    "\n",
    "#    initial_b3 = tf.compat.v1.constant(value=0.0, shape=[NR_CLASSES]) \n",
    "#    b3 = tf.compat.v1.Variable(initial_value=initial_b3, name='b3')\n",
    "\n",
    "#    layer3_in = tf.compat.v1.matmul(layer2_out, w3) + b3\n",
    "#    output = tf.compat.v1.nn.softmax(layer3_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "ad49dba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#b3.eval(sess)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
